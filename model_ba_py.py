# -*- coding: utf-8 -*-
"""model_BA.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1APfCSNS4Wmqa0_sCXHvBckadKKBeyS75
"""

# model.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class BAModel(nn.Module):
    def __init__(self, num_snps=83313, d_model=64, bottleneck_tokens=256, num_heads=2):
        super().__init__()
        # ---------------- MRI Branch (3D CNN) ----------------
        self.mri_cnn = nn.Sequential(
            nn.Conv3d(1, 16, kernel_size=6), nn.ReLU(),
            nn.MaxPool3d(2), nn.BatchNorm3d(16),

            nn.Conv3d(16, 32, kernel_size=6), nn.ReLU(),
            nn.MaxPool3d(2), nn.BatchNorm3d(32), nn.Dropout(0.2),

            nn.Conv3d(32, 64, kernel_size=6), nn.ReLU(),
            nn.MaxPool3d(2), nn.BatchNorm3d(64), nn.Dropout(0.2),

            nn.Conv3d(64, 128, kernel_size=6), nn.ReLU(),
            nn.MaxPool3d(2), nn.BatchNorm3d(128), nn.Dropout(0.2),

            nn.AdaptiveAvgPool3d(1),
        )
        self.mri_fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128, 128), nn.ReLU(), nn.Dropout(0.3)
        )

        # ---------------- SNP Branch (Bottleneck Transformer) ----------------
        self.snp_fc_in = nn.Linear(1, d_model)
        self.pos_emb = nn.Embedding(num_snps, d_model)
        self.bottleneck_tokens = nn.Parameter(torch.randn(1, bottleneck_tokens, d_model))
        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)
        self.ln1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, 128), nn.ReLU(),
            nn.Linear(128, d_model)
        )
        self.ln2 = nn.LayerNorm(d_model)
        self.snp_fc_out = nn.Sequential(
            nn.Linear(d_model, 128), nn.ReLU(), nn.Dropout(0.3)
        )

        # ---------------- Embeddings & Final Fusion ----------------
        self.mri_embed_fc = nn.Linear(128, 128)
        self.snp_embed_fc = nn.Linear(128, 128)

        self.fusion_fc = nn.Sequential(
            nn.Linear(128 + 128, 128), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(128, 1)  # age output
        )

    def forward(self, mri, snp):
        # MRI branch
        x_mri = self.mri_cnn(mri)
        x_mri = self.mri_fc(x_mri)
        mri_embed = F.relu(self.mri_embed_fc(x_mri))

        # SNP branch
        snp = snp.unsqueeze(-1)  # (B, N, 1)
        snp = self.snp_fc_in(snp)  # (B, N, d_model)
        pos = torch.arange(0, snp.size(1), device=snp.device)
        snp = snp + self.pos_emb(pos)

        b_tokens = self.bottleneck_tokens.expand(snp.size(0), -1, -1).to(snp.device) # batch tile
        attn_out, _ = self.attn(b_tokens, snp, snp)
        attn_out = self.ln1(attn_out + b_tokens)
        ff_out = self.ff(attn_out)
        ff_out = self.ln2(ff_out + attn_out)
        snp_x = ff_out.mean(dim=1)  # GlobalAveragePooling1D
        snp_x = self.snp_fc_out(snp_x)
        snp_embed = F.relu(self.snp_embed_fc(snp_x))

        # Fusion
        combined = torch.cat([x_mri, snp_x], dim=1)
        age = self.fusion_fc(combined)

        return {"age": age, "mri_embed": mri_embed, "snp_embed": snp_embed}