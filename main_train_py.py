# -*- coding: utf-8 -*-
"""main_train.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N3QLXnDYyTbgnamuRPE6XwSXC8bUN3Hg
"""

# main_train.py
import os
import numpy as np
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader
from torch import optim

from model import BAModel
from loss_utils import MSEWithAgeContrastive
from train_utils import BrainDataset, save_mae_fig, save_loss_fig, save_checkpoint

# ---------------- Settings ----------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Data paths (replace with actual paths)
loc = "/"
brains_file = "/.npy"
cas_file = "/.npy"
snps_file = "/.npy"
save_loc = "/"
os.makedirs(save_loc, exist_ok=True)

# Hyperparameters
epochs = 500
pretrain_epochs = 30
batch_size = 16
lr = 1e-3
tau = 0.07
max_samples = 15019
test_size = 0.15
num_snps = 83313

# ---------------- Load Data ----------------
all_brains = np.load(os.path.join(loc, brains_file), mmap_mode="r").astype(np.float32)
cas = np.load(os.path.join(loc, cas_file)).reshape(-1,1).astype(np.float32)
snps = np.load(os.path.join(loc, snps_file), mmap_mode="r")[:,1:].astype(np.float32)
print("Data shapes:", all_brains.shape, cas.shape, snps.shape)

# ---------------- Train/Val Split ----------------
indices = np.arange(max_samples)
train_indices, val_indices = train_test_split(indices, test_size=test_size, random_state=75)

train_dataset = BrainDataset(all_brains, snps, cas, train_indices)
val_dataset = BrainDataset(all_brains, snps, cas, val_indices)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

# ---------------- Model ----------------
model = BAModel(num_snps=num_snps).to(device)
optimizer = optim.Adam(model.parameters(), lr=lr)
mse_loss_fn = torch.nn.MSELoss()

# ---------------- Unified Evaluation ----------------
def evaluate(model, data_loader, loss_fn):
    """Compute average loss and MAE on validation set."""
    model.eval()
    total_loss, total_mae, count = 0, 0, 0
    with torch.no_grad():
        for mri, snp, label in data_loader:
            mri, snp, label = mri.to(device), snp.to(device), label.to(device)
            outputs = model(mri, snp)
            age_pred = outputs["age"]
            mri_embed = outputs["mri_embed"]
            snp_embed = outputs["snp_embed"]

            # Try combined loss; fallback to MSE if TypeError
            try:
                loss = loss_fn(age_pred, label, mri_embed, snp_embed)
            except TypeError:
                loss = loss_fn(age_pred, label)

            mae = torch.mean(torch.abs(age_pred - label))
            total_loss += loss.item() * label.size(0)
            total_mae += mae.item() * label.size(0)
            count += label.size(0)

    return total_loss / count, total_mae / count

# ---------------- Pretraining ----------------
print("Starting pretraining (MSE only)...")
for epoch in range(pretrain_epochs):
    model.train()
    epoch_loss, epoch_mae, count = 0, 0, 0
    for mri, snp, label in train_loader:
        mri, snp, label = mri.to(device), snp.to(device), label.to(device)
        optimizer.zero_grad()
        outputs = model(mri, snp)
        age_pred = outputs["age"]

        loss = mse_loss_fn(age_pred, label)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * label.size(0)
        epoch_mae += torch.sum(torch.abs(age_pred - label)).item()
        count += label.size(0)

    train_loss = epoch_loss / count
    train_mae = epoch_mae / count
    val_loss, val_mae = evaluate(model, val_loader, mse_loss_fn)

    # --- Append to history ---
    history["train_loss"].append(train_loss)
    history["val_loss"].append(val_loss)
    history["train_mae"].append(train_mae)
    history["val_mae"].append(val_mae)

    print(f"[Pretrain Epoch {epoch+1}/{pretrain_epochs}] Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f}")
    save_checkpoint(model, optimizer, epoch+1, save_loc, "pretrain")

# ---------------- Fine-tuning ----------------
age_cl_loss_fn = MSEWithAgeContrastive(
    lambda_m2s=0.2,
    lambda_s2m=0.2,
    lambda_m2m=0.2,
    lambda_s2s=0.1,
    tau=tau
)
print("Starting fine-tuning (MSE + Age-guided Contrastive Loss)...")
for epoch in range(pretrain_epochs, epochs):
    model.train()
    epoch_loss, epoch_mae, count = 0, 0, 0
    for mri, snp, label in train_loader:
        mri, snp, label = mri.to(device), snp.to(device), label.to(device)
        optimizer.zero_grad()
        outputs = model(mri, snp)
        age_pred = outputs["age"]
        mri_embed = outputs["mri_embed"]
        snp_embed = outputs["snp_embed"]

        loss = age_cl_loss_fn(age_pred, label, mri_embed, snp_embed)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * label.size(0)
        epoch_mae += torch.sum(torch.abs(age_pred - label)).item()
        count += label.size(0)

    train_loss = epoch_loss / count
    train_mae = epoch_mae / count
    val_loss, val_mae = evaluate(model, val_loader, age_cl_loss_fn)

    # --- Append to history ---
    history["train_loss"].append(train_loss)
    history["val_loss"].append(val_loss)
    history["train_mae"].append(train_mae)
    history["val_mae"].append(val_mae)

    print(f"[Fine-tune Epoch {epoch+1}/{epochs}] Train MAE: {train_mae:.4f} | Val MAE: {val_mae:.4f}")
    save_checkpoint(model, optimizer, epoch+1, save_loc, "finetune")

# ---------------- Save Figures ----------------
history = {
    "train_loss": [], "val_loss": [],
    "train_mae": [], "val_mae": []
}
save_mae_fig(history, os.path.join(save_loc, "MAE_history.jpg"))
save_loss_fig(history, os.path.join(save_loc, "Loss_history.jpg"))
print(f"âœ… Training completed.")